{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python library imports:\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer, TweetTokenizer\n",
    "np.random.seed(seed=234)\n",
    "# The below reads in N lines of text from the 40-million-word news corpus I used (provided by SwiftKey for educational purposes).\n",
    "N = 10000\n",
    "with open(\"news.txt\") as myfile:\n",
    "    articles = [next(myfile) for x in range(N)]\n",
    "joined_articles = [\" \".join(articles)]\n",
    "# The below takes out anything that's not a letter, replacing it with a space, as well as any single letter that is not the pronoun \"I\" or the article \"a.\"\n",
    "def clean_article(article):\n",
    "    art1 = re.sub(\"[^A-Za-z]\", ' ', article)\n",
    "    art2 = re.sub(\"\\s[B-HJ-Zb-hj-z]\\s\", ' ', art1)\n",
    "    art3 = re.sub(\"^[B-HJ-Zb-hj-z]\\s\", ' ', art2)\n",
    "    art4 = re.sub(\"\\s[B-HJ-Zb-hj-z]$\", ' ', art3)\n",
    "    return art4.lower()\n",
    "# The below breaks up the words into n-grams of length 1 to 5 and puts their counts into a Pandas dataframe with the n-grams as column names.  The maximum number of n-grams can be specified if a large corpus is being used.\n",
    "ngram_bow = CountVectorizer(stop_words = None, preprocessor = clean_article, tokenizer = WhitespaceTokenizer().tokenize, ngram_range=(1,5), max_features = None, max_df = 1.0, min_df = 1, binary = False)\n",
    "ngram_count_sparse = ngram_bow.fit_transform(joined_articles)\n",
    "ngram_count = pd.DataFrame(ngram_count_sparse.toarray())\n",
    "ngram_count.columns = ngram_bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'national']\n",
      "New l\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assessment      0.166667\n",
       "championship    0.166667\n",
       "invitation      0.166667\n",
       "press           0.166667\n",
       "spotlight       0.333333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The below turns the n-gram-count dataframe into a Pandas series with the n-grams as indices for ease of working with the counts.  The second line can be used to limit the n-grams used to those with a count over a cutoff value.\n",
    "sums = ngram_count.sum(axis = 0)\n",
    "sums = sums[sums > 0]\n",
    "ngrams = list(sums.index.values)\n",
    "# The function below gives the total number of occurrences of 1-grams in order to calculate 1-gram frequencies\n",
    "def number_of_onegrams(sums):\n",
    "    onegrams = 0\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == 1:\n",
    "            onegrams += sums[ng]\n",
    "    return onegrams\n",
    "# The function below makes a series of 1-gram frequencies.  This is the last resort of the back-off algorithm if the n-gram completion does not occur in the corpus with any of the prefix words.\n",
    "def base_freq(og):\n",
    "    freqs = pd.Series()\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == 1:\n",
    "            freqs[ng] = sums[ng] / og\n",
    "    return freqs\n",
    "# For use in later functions so as not to re-calculate multiple times:\n",
    "bf = base_freq(number_of_onegrams(sums))\n",
    "# The function below finds any n-grams that are completions of a given prefix phrase with a specified number (could be zero) of words 'chopped' off the beginning.  For each, it calculates the count ratio of the completion to the (chopped) prefix, tabulating them in a series to be returned by the function.  If the number of chops equals the number of words in the prefix (i.e. all prefix words are chopped), the 1-gram base frequencies are returned.\n",
    "def find_completion_scores(prefix, chops, factor = 0.4):\n",
    "    cs = pd.Series()\n",
    "    prefix_split = prefix.split(\" \")\n",
    "    l = len(prefix_split)\n",
    "    prefix_split_chopped = prefix_split[chops:l]\n",
    "    #print(prefix_split_chopped)\n",
    "    new_l = l - chops\n",
    "    #print(\"New l\")\n",
    "    #print(new_l)\n",
    "    if new_l == 0:\n",
    "        return factor**chops * bf\n",
    "    prefix_chopped = ' '.join(prefix_split_chopped)\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if (len(ng_split) == new_l + 1) and (ng_split[0:new_l] == prefix_split_chopped):\n",
    "            cs[ng_split[-1]] = factor**chops * sums[ng] / sums[prefix_chopped]\n",
    "    return cs\n",
    "# Example of completion scores:\n",
    "find_completion_scores('in the national', 0, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'the', 'big']\n",
      "New l\n",
      "3\n",
      "['the', 'big']\n",
      "New l\n",
      "2\n",
      "['big']\n",
      "New l\n",
      "1\n",
      "[]\n",
      "New l\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "picture    64.7\n",
       "east        2.0\n",
       "screen      1.3\n",
       "leagues     1.3\n",
       "party       1.3\n",
       "ten         1.3\n",
       "sky         0.7\n",
       "spoked      0.7\n",
       "rocker      0.7\n",
       "push        0.7\n",
       "five        0.7\n",
       "ou          0.7\n",
       "news        0.7\n",
       "lump        0.7\n",
       "house       0.7\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The below tries different numbers of 'chops' up to the length of the prefix to come up with a (still unordered) combined list of scores for potential completions of the prefix.\n",
    "def score_given(given, fact = 0.4):\n",
    "    sg = pd.Series()\n",
    "    given_split = given.split(\" \")\n",
    "    given_length = len(given_split)\n",
    "    for i in range(given_length+1):\n",
    "        fcs = find_completion_scores(given, i, fact)\n",
    "        for i in fcs.index:\n",
    "            if i not in sg.index:\n",
    "                sg[i] = fcs[i]\n",
    "    return sg\n",
    "#The below takes the potential completion scores, puts them in descending order and re-normalizes them as a percentage (pseudo-probability).\n",
    "def score_output(given, fact = 0.4):\n",
    "    sg = score_given(given, fact)\n",
    "    ss = sg.sum()\n",
    "    sg = 100 * sg / ss\n",
    "    sg.sort_values(axis=0, ascending=False, inplace=True)\n",
    "    return round(sg,1)\n",
    "# The below shows that even with a corpus that is way too small (only 1.7 million words vs. billions or even trillions in current research), results start to become intuitive.\n",
    "score_output('on the big', fact = 0.4)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
